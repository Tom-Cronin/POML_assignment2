{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT5170: Principles of ML - Assignment 2\n",
    "## Course code: 1MAO3\n",
    "### Participants (name: id): [ Daniel Verdejo: 22240224, Tom Cronin: < id >]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: DELETE THIS - ITS SOME BASIC INFORMATION AS A REFERENCE \n",
    "\n",
    "## Definition of the perceptron\n",
    "The perceptron is a linear-model binary classifier with a simple input—output relationship as depicted in Figure 2-3, which shows we're summing n number of inputs times their associated weights and then sending this \"net input\" to a step function with a defined threshold. Typically with perceptrons, this is a Heaviside step function with a threshold value of 0.5. This function will output a real-valued single binary value (0 or a 1), depending on the input.\n",
    "\n",
    "![perceptron](assets/perceptronFig.png?raw=true)\n",
    "\n",
    "We can model the decision boundary and the classification output in the Heaviside\n",
    "step function equation, as follows:\n",
    "\n",
    "![f of x](assets/fx.png)\n",
    "\n",
    "To produce the net input to the activation function (here, the Heaviside step function) we take the dot product of the input and the connection weights. We see this\n",
    "summation in the left half of Figure 2-3 as the input to the summation function.\n",
    "Table 2-1 provides an explanation of how the summation function is performed as\n",
    "well as notes about the parameters involved in the summation function.\n",
    "\n",
    "\n",
    "![params](assets/params.png)\n",
    "\n",
    "The output of the step function (activation function) is the output for the perceptron and gives us a classification of the input values. If the bias value is negative, it forces the learned weights sum to be a much greater value to get a 1 classification output. The bias term in this capacity moves the decision boundary around for the model. Input values do not affect the bias term, but the bias term is learned through the perceptron learning algorithm.\n",
    "\n",
    "## The perceptron learning algorithm\n",
    "The perceptron learning algorithm changes the weights in the perceptron model until all input records are all correctly classified. The algorithm will not terminate if the learning input is not linearly separable. A linearly separable dataset is one for which we can find the values of a hyperplane that will cleanly divide the two classes of the dataset.\n",
    "\n",
    "The perceptron learning algorithm initializes the weight vector with small random values or 0.0s at the beginning of training. The perceptron learning algorithm takes each input record, as we can see in Figure 2-3, and computes the output classification to check against the actual classification label. To produce the classification, the columns (features) are matched up to weights where n is the number of dimensions in both our input and weights. The first input value is the bias input, which is always 1.0 because we don't affect the bias input. The first weight is our bias term in this diagram. The dot product of the input vector and the weight vector gives us the input to our activation function, as we've previously discussed.\n",
    "\n",
    "If the classification is correct, no weight changes are made. If the classification is incorrect, the weights are adjusted accordingly. Weights are updated between individual training examples in an \"online learning\" fashion. This loop continues until all of the input examples are correctly classified. If the dataset is not linearly separable, the training algorithm will not terminate. Figure 2-4 demonstrates a dataset that is not linearly separable, the XOR logic function.\n",
    "\n",
    "![xor](assets/xor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import doctest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tom Cronin\n",
    "def read_data_return_dataframe(PathToFile):\n",
    "    \"\"\"\n",
    "    Should return a dataframe of the test file\n",
    "    >>> type(read_data_return_dataframe('testdata.txt'))\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "\n",
    "     Should have correct data in the dataframe.\n",
    "     There should be 20 entrys in the dataframe excluding of the column names\n",
    "    >>> read_data_return_dataframe('testdata.txt').size\n",
    "    20\n",
    "    \"\"\"\n",
    "    return pd.read_table(PathToFile) # reads txt file and converts it to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    type(read_data_return_dataframe('testdata.txt'))\n",
      "Expecting:\n",
      "    <class 'pandas.core.frame.DataFrame'>\n",
      "ok\n",
      "Trying:\n",
      "    read_data_return_dataframe('testdata.txt').size\n",
      "Expecting:\n",
      "    20\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(read_data_return_dataframe, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Verdejo - split into labels and attributes\n",
    "def split_df_labels_attributes(df):\n",
    "    \"\"\" Split the dataframe into two by labels and attributes\n",
    "\n",
    "        Keyword arguments:\n",
    "        df -- A pandas dataframe type containing labels and attributes\n",
    "        label_col_name -- A string which contains the name of the label column. \n",
    "\n",
    "        Returns:\n",
    "        tuple -- (label: pd.DataFrame, attributes: pd.DataFrame)\n",
    "\n",
    "        Should output only the Label column\n",
    "        >>> labels, attrs = split_df_labels_attributes(read_data_return_dataframe('testdata.txt'))\n",
    "        >>> labels\n",
    "                  Label\n",
    "                0  no   \n",
    "                1  no\n",
    "\n",
    "        Should not contain the Label column\n",
    "        >>> labels, attrs = split_df_labels_attributes(read_data_return_dataframe('testdata-alt.txt'))\n",
    "        >>> 'label' in attrs\n",
    "        False\n",
    "        \n",
    "        >>> attrs.columns\n",
    "        Index(['year', 'temp', 'humidity', 'rainfall', 'drought_code', 'buildup_index',\n",
    "          'day', 'month', 'wind_speed'],\n",
    "        dtype='object')\n",
    "      \"\"\"\n",
    "    return (df.iloc[:,0:1], df.iloc[:,1:])  # (for every row take columns upto index 1 exclusive, for every row take every column from 1 onwards inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    labels, attrs = split_df_labels_attributes(read_data_return_dataframe('testdata.txt'))\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    labels\n",
      "Expecting:\n",
      "              Label\n",
      "            0  no   \n",
      "            1  no\n",
      "ok\n",
      "Trying:\n",
      "    labels, attrs = split_df_labels_attributes(read_data_return_dataframe('testdata-alt.txt'))\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    'label' in attrs\n",
      "Expecting:\n",
      "    False\n",
      "ok\n",
      "Trying:\n",
      "    attrs.columns\n",
      "Expecting:\n",
      "    Index(['year', 'temp', 'humidity', 'rainfall', 'drought_code', 'buildup_index',\n",
      "      'day', 'month', 'wind_speed'],\n",
      "    dtype='object')\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(split_df_labels_attributes, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Verdejo - train, test splilt dataframe\n",
    "def split_df_to_train_test_dfs(df):\n",
    "    \"\"\" Splits a single dataframe into 2 dataframes\n",
    "    \n",
    "    Arguments:\n",
    "    df -- A pandas Dataframe to be split into 2\n",
    "\n",
    "    Returns:\n",
    "    tuple -- (df_train: pandas.Dataframe, df_test: pandas.Dataframe)\n",
    "    \n",
    "    Should split a single dataframe into 2 unique dataframes\n",
    "    >>> df = read_data_return_dataframe('wildfires.txt')\n",
    "    >>> train, test = split_df_to_train_test_dfs(df)\n",
    "    >>> train.index.equals(tr.index)\n",
    "    False\n",
    "\n",
    "    Length of train and test dataframes should equal the length of the orignal\n",
    "    >>> df = read_data_return_dataframe('wildfires.txt')\n",
    "    >>> train, test = split_df_to_train_test_dfs(df)\n",
    "    >>> len(train) + len(test) == len(df)\n",
    "    True\n",
    "\n",
    "    Should contain different values\n",
    "    >>> df = read_data_return_dataframe('wildfires.txt')\n",
    "    >>> train, test = split_df_to_train_test_dfs(df)\n",
    "    >>> train.values != test.values\n",
    "    True\n",
    "    \"\"\"\n",
    "    train_frac = round(np.random.uniform(.6, .7), 2) # get a random float for our training fraction\n",
    "    df_train = df.sample(frac = train_frac) # randomly sample a fraction of the dataframe between 60 & 70 % of its entirety\n",
    "    return (df_train,  df.drop(df_train.index)) # return the training data and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    df = read_data_return_dataframe('wildfires.txt')\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    train, test = split_df_to_train_test_dfs(df)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    train.index.equals(tr.index)\n",
      "Expecting:\n",
      "    False\n",
      "ok\n",
      "Trying:\n",
      "    df = read_data_return_dataframe('wildfires.txt')\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    train, test = split_df_to_train_test_dfs(df)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    len(train) + len(test) == len(df)\n",
      "Expecting:\n",
      "    True\n",
      "ok\n",
      "Trying:\n",
      "    df = read_data_return_dataframe('wildfires.txt')\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    train, test = split_df_to_train_test_dfs(df)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    train.values != test.values\n",
      "Expecting:\n",
      "    True\n",
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<doctest NoName[8]>:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  train.values != test.values\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(split_df_to_train_test_dfs, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daniel Verdejo - initial perceptron based on description above \n",
    "# TODO : create a fn to normalise the outputs! THEN change this test to use the real y!!! ¯\\_(ツ)_/¯\n",
    "# TODO : The predict is also fucked right now ¯\\_(ツ)_/¯\n",
    "# TODO : Figure out if fit_predict and eval are needed ¯\\_(ツ)_/¯\n",
    "class Perceptron:\n",
    "    \"\"\" The perceptron class \n",
    "\n",
    "    should return am imstamce of the created object\n",
    "    >>> Perceptron(learn_rate=0.1, n_iters=100)\n",
    "    Perceptron()\n",
    "\n",
    "    should return self (an instance of the object)\n",
    "    >>> df = read_data_return_dataframe(\"./testdata-alt.txt\")\n",
    "    >>> y, X = split_df_labels_attributes(df)\n",
    "    >>> Perceptron(learn_rate=0.1, n_iters=100).fit(X, np.array([0,0,1,0,1,1]))\n",
    "    Perceptron()\n",
    "\n",
    "    should return the prediction\n",
    "    >>> df = read_data_return_dataframe(\"./testdata-alt.txt\")\n",
    "    >>> y, X = split_df_labels_attributes(df)\n",
    "    >>> _, X_test = split_df_labels_attributes(read_data_return_dataframe('./testdata.txt'))\n",
    "    >>> P = Perceptron(learn_rate=0.1, n_iters=100).fit(X, np.array([0,0,1,0,1,1]))\n",
    "    >>> pred = P.predict(X_test)\n",
    "    [0, 0] \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learn_rate, n_iters):\n",
    "        self.learn_rate = learn_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.activation_fn = (lambda x: np.where(x >= 0, 1, 0))\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{type(self).__name__}()\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        if(isinstance(X, pd.DataFrame)):\n",
    "            X = X.to_numpy()\n",
    "        \n",
    "        self.weights = np.random.rand(X.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for i, x_i in enumerate(X):\n",
    "                ln_out = np.dot(x_i, self.weights) + self.bias\n",
    "                pred = self.activation_fn(ln_out)\n",
    "                update = self.learn_rate * (y[i] - pred)\n",
    "\n",
    "                self.weights = self.weights + (update * x_i)\n",
    "                self.bias += update\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if(isinstance(X, pd.DataFrame)):\n",
    "            X = X.to_numpy()\n",
    "\n",
    "        ln_out = np.dot(X, self.weights) + self.bias\n",
    "        pred = self.activation_fn(ln_out)\n",
    "        print(ln_out)\n",
    "        return pred\n",
    "\n",
    "\n",
    "    # Not sure if these are needed yet\n",
    "    def fit_predict(self):\n",
    "        return self\n",
    "    \n",
    "    def evaluate(self):\n",
    "        pass\n",
    "\n",
    "# TODO: remove these when done testing! \n",
    "# df = read_data_return_dataframe(\"./testdata-alt.txt\")\n",
    "# df_train, df_test = split_df_to_train_test_dfs(df)\n",
    "# y_train, X_train = split_df_labels_attributes(df_train)\n",
    "\n",
    "# y_train = [1 if y == 'yes' else 0 for y in y_train['label']]\n",
    "\n",
    "# y_test, X_test = split_df_labels_attributes(df_test)\n",
    "\n",
    "# P = Perceptron(.05, 5).fit(X_train, y_train)\n",
    "# P\n",
    "# # # pred = P.predict(X_test)\n",
    "# # pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    Perceptron(learn_rate=0.1, n_iters=100)\n",
      "Expecting:\n",
      "    Perceptron()\n",
      "ok\n",
      "Trying:\n",
      "    df = read_data_return_dataframe(\"./testdata-alt.txt\")\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    y, X = split_df_labels_attributes(df)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    Perceptron(learn_rate=0.1, n_iters=100).fit(X, np.array([0,0,1,0,1,1]))\n",
      "Expecting:\n",
      "    Perceptron()\n",
      "ok\n",
      "Trying:\n",
      "    df = read_data_return_dataframe(\"./testdata-alt.txt\")\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    y, X = split_df_labels_attributes(df)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    _, X_test = split_df_labels_attributes(read_data_return_dataframe('./testdata.txt'))\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    P = Perceptron(learn_rate=0.1, n_iters=100).fit(X, np.array([0,0,1,0,1,1]))\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    pred = P.predict(X_test)\n",
      "Expecting:\n",
      "    0\n",
      "**********************************************************************\n",
      "File \"__main__\", line ?, in NoName\n",
      "Failed example:\n",
      "    pred = P.predict(X_test)\n",
      "Expected:\n",
      "    0\n",
      "Got:\n",
      "    [93778.91550826 94001.51597795]\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(Perceptron, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![entropy](assets/entropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Entropy calculation - need to figure out whats really required to check what causes a fire or not\n",
    "# Presuming the date isn't really all that important, I believe the attributes of interest are:\n",
    "# temp, humidity, rainfall, drought_code, buildup_index, wind_speed\n",
    "# For each outcome, (yes / no in this case) sum all of the positive and negative cases to the first algorithm above \n",
    "# page 139 - ref: https://drive.google.com/drive/folders/1U6F01iWnnSTrJQzviE02ST8WMc8UdOML\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\" Entropy - for a set of data summate all positive cases, and negative cases:\n",
    "        for each case calculate p * log2 p (log base 2) if p is a non negative number\n",
    "\n",
    "        Should return 1.0 if there label distribution is equal\n",
    "        >>> round(entropy(np.array([1,1,1,1,0,0,0,0])),3)\n",
    "        1.0\n",
    "\n",
    "        Should return 0.0 if the labels are one sided negatively\n",
    "        >>> Y = np.array([0,0,0,0,0,0,0,0,0])\n",
    "        >>> round(entropy(Y),3)\n",
    "        -0.0\n",
    "        \n",
    "        Should return 0.0 if the labels are one sided positively\n",
    "        >>> Y = np.array([1,1,1,1,1])\n",
    "        >>> round(entropy(Y),3)\n",
    "        -0.0\n",
    "\n",
    "        Should return a value close to 1.0 if the labels are almost equal\n",
    "        >>> Y = np.array([1, 0, 0, 0, 1, 1, 0 ,1 ,0, 0, 1, 0, 1])\n",
    "        >>> round(entropy(Y), 3)\n",
    "        0.996\n",
    "\n",
    "        Should return a float closer to 0.0 if the labels are very unbalanced\n",
    "        >>> Y = np.array([1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "        >>> round(entropy(Y),3)\n",
    "        0.286\n",
    "    \"\"\"\n",
    "    P = np.bincount(y) / len(y) # number of unique outputs / all possible outputs\n",
    "    return - np.sum([p * np.log2(p) for p in P if p > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    round(entropy(np.array([1,1,1,1,0,0,0,0])),3)\n",
      "Expecting:\n",
      "    1.0\n",
      "ok\n",
      "Trying:\n",
      "    Y = np.array([0,0,0,0,0,0,0,0,0])\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    round(entropy(Y),3)\n",
      "Expecting:\n",
      "    -0.0\n",
      "ok\n",
      "Trying:\n",
      "    Y = np.array([1,1,1,1,1])\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    round(entropy(Y),3)\n",
      "Expecting:\n",
      "    -0.0\n",
      "ok\n",
      "Trying:\n",
      "    Y = np.array([1, 0, 0, 0, 1, 1, 0 ,1 ,0, 0, 1, 0, 1])\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    round(entropy(Y), 3)\n",
      "Expecting:\n",
      "    0.996\n",
      "ok\n",
      "Trying:\n",
      "    Y = np.array([1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    round(entropy(Y),3)\n",
      "Expecting:\n",
      "    0.286\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(entropy, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c69e896b73d80df03b10aefe902562c227bdab9e6e1527e46fe261fc763f811"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
