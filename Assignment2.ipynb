{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT5170: Principles of ML - Assignment 2\n",
    "## Course code: 1MAO3\n",
    "### Participants (name: id): [ Daniel Verdejo: 22240224, Tom Cronin: < id >]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: DELETE THIS - ITS SOME BASIC INFORMATION AS A REFERENCE \n",
    "\n",
    "## Definition of the perceptron\n",
    "The perceptron is a linear-model binary classifier with a simple input—output relationship as depicted in Figure 2-3, which shows we're summing n number of inputs times their associated weights and then sending this \"net input\" to a step function with a defined threshold. Typically with perceptrons, this is a Heaviside step function with a threshold value of 0.5. This function will output a real-valued single binary value (0 or a 1), depending on the input.\n",
    "\n",
    "![perceptron](assets/perceptronFig.png?raw=true)\n",
    "\n",
    "We can model the decision boundary and the classification output in the Heaviside\n",
    "step function equation, as follows:\n",
    "\n",
    "![f of x](assets/fx.png)\n",
    "\n",
    "To produce the net input to the activation function (here, the Heaviside step function) we take the dot product of the input and the connection weights. We see this\n",
    "summation in the left half of Figure 2-3 as the input to the summation function.\n",
    "Table 2-1 provides an explanation of how the summation function is performed as\n",
    "well as notes about the parameters involved in the summation function.\n",
    "\n",
    "\n",
    "![params](assets/params.png)\n",
    "\n",
    "The output of the step function (activation function) is the output for the perceptron and gives us a classification of the input values. If the bias value is negative, it forces the learned weights sum to be a much greater value to get a 1 classification output. The bias term in this capacity moves the decision boundary around for the model. Input values do not affect the bias term, but the bias term is learned through the perceptron learning algorithm.\n",
    "\n",
    "## The perceptron learning algorithm\n",
    "The perceptron learning algorithm changes the weights in the perceptron model until all input records are all correctly classified. The algorithm will not terminate if the learning input is not linearly separable. A linearly separable dataset is one for which we can find the values of a hyperplane that will cleanly divide the two classes of the dataset.\n",
    "\n",
    "The perceptron learning algorithm initializes the weight vector with small random values or 0.0s at the beginning of training. The perceptron learning algorithm takes each input record, as we can see in Figure 2-3, and computes the output classification to check against the actual classification label. To produce the classification, the columns (features) are matched up to weights where n is the number of dimensions in both our input and weights. The first input value is the bias input, which is always 1.0 because we don't affect the bias input. The first weight is our bias term in this diagram. The dot product of the input vector and the weight vector gives us the input to our activation function, as we've previously discussed.\n",
    "\n",
    "If the classification is correct, no weight changes are made. If the classification is incorrect, the weights are adjusted accordingly. Weights are updated between individual training examples in an \"online learning\" fashion. This loop continues until all of the input examples are correctly classified. If the dataset is not linearly separable, the training algorithm will not terminate. Figure 2-4 demonstrates a dataset that is not linearly separable, the XOR logic function.\n",
    "\n",
    "![xor](assets/xor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import doctest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Potentially remove this if we don't actually need it\n",
    "# Entropy calculation - need to figure out whats really required to check what causes a fire or not\n",
    "# Presuming the date isn't really all that important, I believe the attributes of interest are:\n",
    "# temp, humidity, rainfall, drought_code, buildup_index, wind_speed\n",
    "# For each outcome, (yes / no in this case) sum all of the positive and negative cases to the first algorithm above \n",
    "# page 139 - ref: https://drive.google.com/drive/folders/1U6F01iWnnSTrJQzviE02ST8WMc8UdOML\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\" Entropy - for a set of data summate all positive cases, and negative cases:\n",
    "        for each case calculate p * log2 p (log base 2) if p is a non negative number\n",
    "\n",
    "        Should return 1.0 if there label distribution is equal\n",
    "        >>> round(entropy(np.array([1,1,1,1,0,0,0,0])),3)\n",
    "        1.0\n",
    "\n",
    "        Should return 0.0 if the labels are one sided negatively\n",
    "        >>> Y = np.array([0,0,0,0,0,0,0,0,0])\n",
    "        >>> round(entropy(Y),3)\n",
    "        -0.0\n",
    "        \n",
    "        Should return 0.0 if the labels are one sided positively\n",
    "        >>> Y = np.array([1,1,1,1,1])\n",
    "        >>> round(entropy(Y),3)\n",
    "        -0.0\n",
    "\n",
    "        Should return a value close to 1.0 if the labels are almost equal\n",
    "        >>> Y = np.array([1, 0, 0, 0, 1, 1, 0 ,1 ,0, 0, 1, 0, 1])\n",
    "        >>> round(entropy(Y), 3)\n",
    "        0.996\n",
    "\n",
    "        Should return a float closer to 0.0 if the labels are very unbalanced\n",
    "        >>> Y = np.array([1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "        >>> round(entropy(Y),3)\n",
    "        0.286\n",
    "    \"\"\"\n",
    "    P = np.bincount(y) / len(y) # number of unique outputs / all possible outputs\n",
    "    return - np.sum([p * np.log2(p) for p in P if p > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    round(entropy(np.array([1,1,1,1,0,0,0,0])),3)\n",
      "Expecting:\n",
      "    1.0\n",
      "ok\n",
      "Trying:\n",
      "    Y = np.array([0,0,0,0,0,0,0,0,0])\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    round(entropy(Y),3)\n",
      "Expecting:\n",
      "    -0.0\n",
      "ok\n",
      "Trying:\n",
      "    Y = np.array([1,1,1,1,1])\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    round(entropy(Y),3)\n",
      "Expecting:\n",
      "    -0.0\n",
      "ok\n",
      "Trying:\n",
      "    Y = np.array([1, 0, 0, 0, 1, 1, 0 ,1 ,0, 0, 1, 0, 1])\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    round(entropy(Y), 3)\n",
      "Expecting:\n",
      "    0.996\n",
      "ok\n",
      "Trying:\n",
      "    Y = np.array([1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    round(entropy(Y),3)\n",
      "Expecting:\n",
      "    0.286\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(entropy, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tom Cronin\n",
    "def read_data_return_dataframe(PathToFile):\n",
    "    \"\"\"\n",
    "    Should return a dataframe of the test file\n",
    "    >>> type(read_data_return_dataframe('testdata.txt'))\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "\n",
    "     Should have correct data in the dataframe.\n",
    "     There should be 20 entrys in the dataframe excluding of the column names\n",
    "    >>> read_data_return_dataframe('testdata.txt').size\n",
    "    20\n",
    "    \"\"\"\n",
    "    return pd.read_table(PathToFile) # reads txt file and converts it to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    type(read_data_return_dataframe('testdata.txt'))\n",
      "Expecting:\n",
      "    <class 'pandas.core.frame.DataFrame'>\n",
      "ok\n",
      "Trying:\n",
      "    read_data_return_dataframe('testdata.txt').size\n",
      "Expecting:\n",
      "    20\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(read_data_return_dataframe, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Verdejo - split into labels and attributes\n",
    "def split_df_labels_attributes(df):\n",
    "    \"\"\" Split the dataframe into two by labels and attributes\n",
    "\n",
    "        Keyword arguments:\n",
    "        df -- A pandas dataframe type containing labels and attributes\n",
    "        label_col_name -- A string which contains the name of the label column. \n",
    "\n",
    "        Returns:\n",
    "        tuple -- (label: pd.DataFrame, attributes: pd.DataFrame)\n",
    "\n",
    "        Should output only the Label column\n",
    "        >>> labels, attrs = split_df_labels_attributes(read_data_return_dataframe('testdata.txt'))\n",
    "        >>> labels\n",
    "                  Label\n",
    "                0  no   \n",
    "                1  no\n",
    "\n",
    "        Should not contain the Label column\n",
    "        >>> labels, attrs = split_df_labels_attributes(read_data_return_dataframe('testdata-alt.txt'))\n",
    "        >>> 'label' in attrs\n",
    "        False\n",
    "        \n",
    "        >>> attrs.columns\n",
    "        Index(['year', 'temp', 'humidity', 'rainfall', 'drought_code', 'buildup_index',\n",
    "          'day', 'month', 'wind_speed'],\n",
    "        dtype='object')\n",
    "      \"\"\"\n",
    "    return (df.iloc[:,0:1], df.iloc[:,1:])  # (for every row take columns upto index 1 exclusive, for every row take every column from 1 onwards inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    labels, attrs = split_df_labels_attributes(read_data_return_dataframe('testdata.txt'))\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    labels\n",
      "Expecting:\n",
      "              Label\n",
      "            0  no   \n",
      "            1  no\n",
      "ok\n",
      "Trying:\n",
      "    labels, attrs = split_df_labels_attributes(read_data_return_dataframe('testdata-alt.txt'))\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    'label' in attrs\n",
      "Expecting:\n",
      "    False\n",
      "ok\n",
      "Trying:\n",
      "    attrs.columns\n",
      "Expecting:\n",
      "    Index(['year', 'temp', 'humidity', 'rainfall', 'drought_code', 'buildup_index',\n",
      "      'day', 'month', 'wind_speed'],\n",
      "    dtype='object')\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(split_df_labels_attributes, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Verdejo - train, test splilt dataframe\n",
    "def split_df_to_train_test_dfs(df):\n",
    "    \"\"\" Splits a single dataframe into 2 dataframes\n",
    "    \n",
    "    Arguments:\n",
    "    df -- A pandas Dataframe to be split into 2\n",
    "\n",
    "    Returns:\n",
    "    tuple -- (df_train: pandas.Dataframe, df_test: pandas.Dataframe)\n",
    "    \n",
    "    Should split a single dataframe into 2 unique dataframes\n",
    "    >>> df = read_data_return_dataframe('wildfires.txt')\n",
    "    >>> train, test = split_df_to_train_test_dfs(df)\n",
    "    >>> train.index.equals(tr.index)\n",
    "    False\n",
    "\n",
    "    Length of train and test dataframes should equal the length of the orignal\n",
    "    >>> df = read_data_return_dataframe('wildfires.txt')\n",
    "    >>> train, test = split_df_to_train_test_dfs(df)\n",
    "    >>> len(train) + len(test) == len(df)\n",
    "    True\n",
    "\n",
    "    Should contain different values\n",
    "    >>> df = read_data_return_dataframe('wildfires.txt')\n",
    "    >>> train, test = split_df_to_train_test_dfs(df)\n",
    "    >>> train.values != test.values\n",
    "    True\n",
    "    \"\"\"\n",
    "    train_frac = round(np.random.uniform(.6, .7), 2) # get a random float for our training fraction\n",
    "    df_train = df.sample(frac = train_frac) # randomly sample a fraction of the dataframe between 60 & 70 % of its entirety\n",
    "    return (df_train,  df.drop(df_train.index)) # return the training data and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    df = read_data_return_dataframe('wildfires.txt')\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    train, test = split_df_to_train_test_dfs(df)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    train.index.equals(tr.index)\n",
      "Expecting:\n",
      "    False\n",
      "**********************************************************************\n",
      "File \"__main__\", line 14, in NoName\n",
      "Failed example:\n",
      "    train.index.equals(tr.index)\n",
      "Exception raised:\n",
      "    Traceback (most recent call last):\n",
      "      File \"c:\\Users\\danny\\anaconda3\\envs\\tensorflow\\lib\\doctest.py\", line 1336, in __run\n",
      "        exec(compile(example.source, filename, \"single\",\n",
      "      File \"<doctest NoName[2]>\", line 1, in <module>\n",
      "        train.index.equals(tr.index)\n",
      "    NameError: name 'tr' is not defined\n",
      "Trying:\n",
      "    df = read_data_return_dataframe('wildfires.txt')\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    train, test = split_df_to_train_test_dfs(df)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    len(train) + len(test) == len(df)\n",
      "Expecting:\n",
      "    True\n",
      "ok\n",
      "Trying:\n",
      "    df = read_data_return_dataframe('wildfires.txt')\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    train, test = split_df_to_train_test_dfs(df)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    train.values != test.values\n",
      "Expecting:\n",
      "    True\n",
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<doctest NoName[8]>:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  train.values != test.values\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(split_df_to_train_test_dfs, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix this test... ¯\\_(ツ)_/¯\n",
    "def normalise_outputs(Y):\n",
    "    \"\"\" Change the outputs from 'yes' and 'no' to 1s and 0s 1 = yes, 0 = no\n",
    "\n",
    "    Args:\n",
    "    Y -- dataframe containing the labels to convert to numerical\n",
    "\n",
    "    Returns:\n",
    "    integer array\n",
    "\n",
    "    >>> normalise_outputs(read_data_return_dataframe(\"./testdata-alt.txt\"))\n",
    "    [0,0,1,0,1,1]\n",
    "    \"\"\"\n",
    "    return [1 if 'yes' in y else 0 for y in Y['fire']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    normalise_outputs(read_data_return_dataframe(\"./testdata-alt.txt\"))\n",
      "Expecting:\n",
      "    [0,0,1,0,1,1]\n",
      "**********************************************************************\n",
      "File \"__main__\", line 11, in NoName\n",
      "Failed example:\n",
      "    normalise_outputs(read_data_return_dataframe(\"./testdata-alt.txt\"))\n",
      "Exception raised:\n",
      "    Traceback (most recent call last):\n",
      "      File \"c:\\Users\\danny\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3800, in get_loc\n",
      "        return self._engine.get_loc(casted_key)\n",
      "      File \"pandas\\_libs\\index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n",
      "      File \"pandas\\_libs\\index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n",
      "      File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "      File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "    KeyError: 'fire'\n",
      "\n",
      "    The above exception was the direct cause of the following exception:\n",
      "\n",
      "    Traceback (most recent call last):\n",
      "      File \"c:\\Users\\danny\\anaconda3\\envs\\tensorflow\\lib\\doctest.py\", line 1336, in __run\n",
      "        exec(compile(example.source, filename, \"single\",\n",
      "      File \"<doctest NoName[0]>\", line 1, in <module>\n",
      "        normalise_outputs(read_data_return_dataframe(\"./testdata-alt.txt\"))\n",
      "      File \"C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_29116\\3388039502.py\", line 14, in normalise_outputs\n",
      "        return [1 if 'yes' in y else 0 for y in Y['fire']]\n",
      "      File \"c:\\Users\\danny\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py\", line 3805, in __getitem__\n",
      "        indexer = self.columns.get_loc(key)\n",
      "      File \"c:\\Users\\danny\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3802, in get_loc\n",
      "        raise KeyError(key) from err\n",
      "    KeyError: 'fire'\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(normalise_outputs, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Daniel Verdejo - initial perceptron based on description above\n",
    "# TODO : Figure out if fit_predict and eval are needed ¯\\_(ツ)_/¯\n",
    "class Perceptron:\n",
    "    \"\"\" The perceptron class \n",
    "\n",
    "    Should fail validation\n",
    "    >>> Perceptron(n_iters=[56,1,2])\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    ValueError: n_iters must be an integer and a natural number\n",
    "\n",
    "    Should fail validation\n",
    "    >>> Perceptron(learn_rate=-1)\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    ValueError: learn_rate must be a float or int greater than 0\n",
    "\n",
    "    should return am imstamce of the created object\n",
    "    >>> Perceptron(learn_rate=0.1, n_iters=100)\n",
    "    Perceptron()\n",
    "\n",
    "    should return self (an instance of the object)\n",
    "    >>> df = read_data_return_dataframe(\"./testdata-alt.txt\")\n",
    "    >>> y, X = split_df_labels_attributes(df)\n",
    "    >>> Perceptron(learn_rate=0.1, n_iters=100).fit(X, np.array([0,0,1,0,1,1]))\n",
    "    Perceptron()\n",
    "\n",
    "    should return the prediction\n",
    "    >>> df = read_data_return_dataframe(\"./testdata-alt.txt\")\n",
    "    >>> y, X = split_df_labels_attributes(df)\n",
    "    >>> _, X_test = split_df_labels_attributes(read_data_return_dataframe('./testdata.txt'))\n",
    "    >>> P = Perceptron(learn_rate=0.5, n_iters=1000).fit(X, np.array([0,0,1,0,1,1]))\n",
    "    >>> P.predict(X_test)\n",
    "    array([0, 0])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learn_rate=0.001, n_iters=500):\n",
    "        self._validate_input_params(learn_rate, n_iters)\n",
    "        self.learn_rate = learn_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.activation_fn = (lambda x: np.where(x >= 0, 1, 0)) # simple step function\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{type(self).__name__}()\"\n",
    "\n",
    "    def _validate_input_params(self, learn_rate, n_iters):\n",
    "        if not isinstance(n_iters, int) or n_iters < 1:\n",
    "            raise ValueError(\"n_iters must be an integer and a natural number\")\n",
    "        if not isinstance(learn_rate, (int, float)) or learn_rate <= 0:\n",
    "            raise ValueError(\"learn_rate must be a float or int greater than 0\")\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        if(isinstance(X, pd.DataFrame)):\n",
    "            X = X.to_numpy()\n",
    "        \n",
    "        self.weights = np.random.rand(X.shape[1])\n",
    "        self.bias = np.random.rand(10)[0]\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for i, x_i in enumerate(X):\n",
    "                ln_out = np.dot(x_i, self.weights) + self.bias\n",
    "                update = self.learn_rate * (y[i] - self.activation_fn(ln_out))\n",
    "\n",
    "                self.weights = self.weights + (update * x_i)\n",
    "                self.bias += update\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if(isinstance(X, pd.DataFrame)):\n",
    "            X = X.to_numpy()\n",
    "        ln_out = np.dot(X, self.weights) + self.bias\n",
    "        pred = self.activation_fn(ln_out)\n",
    "        return pred\n",
    "\n",
    "# TODO: remove these when done testing! \n",
    "# df = read_data_return_dataframe(\"./wildfires.txt\")\n",
    "# df_train, df_test = split_df_to_train_test_dfs(df)\n",
    "# y_train, X_train = split_df_labels_attributes(df_train)\n",
    "\n",
    "# y_train = normalise_outputs(y_train)\n",
    "# print(y_train)\n",
    "# y_test, X_test = split_df_labels_attributes(df_test)\n",
    "# y_test = normalise_outputs(y_test)\n",
    "\n",
    "# P = Perceptron(.5, 1000).fit(X_train, y_train)\n",
    "# print(y_test)\n",
    "# pred = P.predict(X_test)\n",
    "# pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    Perceptron(n_iters=[56,1,2])\n",
      "Expecting:\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: n_iters must be an integer and a natural number\n",
      "ok\n",
      "Trying:\n",
      "    Perceptron(learn_rate=-1)\n",
      "Expecting:\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: learn_rate must be a float or int greater than 0\n",
      "ok\n",
      "Trying:\n",
      "    Perceptron(learn_rate=0.1, n_iters=100)\n",
      "Expecting:\n",
      "    Perceptron()\n",
      "ok\n",
      "Trying:\n",
      "    df = read_data_return_dataframe(\"./testdata-alt.txt\")\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    y, X = split_df_labels_attributes(df)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    Perceptron(learn_rate=0.1, n_iters=100).fit(X, np.array([0,0,1,0,1,1]))\n",
      "Expecting:\n",
      "    Perceptron()\n",
      "ok\n",
      "Trying:\n",
      "    df = read_data_return_dataframe(\"./testdata-alt.txt\")\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    y, X = split_df_labels_attributes(df)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    _, X_test = split_df_labels_attributes(read_data_return_dataframe('./testdata.txt'))\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    P = Perceptron(learn_rate=0.5, n_iters=1000).fit(X, np.array([0,0,1,0,1,1]))\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    P.predict(X_test)\n",
      "Expecting:\n",
      "    array([0, 0])\n",
      "**********************************************************************\n",
      "File \"__main__\", line ?, in NoName\n",
      "Failed example:\n",
      "    P.predict(X_test)\n",
      "Expected:\n",
      "    array([0, 0])\n",
      "Got:\n",
      "    array([1, 1])\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(Perceptron, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1598329193.py, line 95)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [14]\u001b[1;36m\u001b[0m\n\u001b[1;33m    def evaluate(self, X, y):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Multi-layer perceptron\n",
    "class MLP():\n",
    "    \"\"\" A somewhat simple implementation of a multi-layer perceptron\n",
    "\n",
    "    Args:\n",
    "    learn_rate -- float: the learning rate value of the neural net\n",
    "    n_iters -- int: the number of iterations that will be carried out\n",
    "    hidden_layer_size -- tuple: the # of perceptrons / hidden layer, and # of hidden layers to be added\n",
    "\n",
    "\n",
    "    Should fail validation\n",
    "    >>> MLP(n_iters=\"whoops\")\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    ValueError: n_iters must be an integer and a natural number\n",
    "\n",
    "    Should fail validation\n",
    "    >>> MLP(hidden_layer_size=(0,0))\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    ValueError: hidden_layer_size must contain natural numbers of type int\n",
    "\n",
    "    Should fail validation\n",
    "    >>> MLP(learn_rate=0)\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    ValueError: learn_rate must be a float or int greater than 0\n",
    "\n",
    "    Should create an instance of the MLP class\n",
    "    >>> mlp = MLP()\n",
    "    >>> mlp\n",
    "    MLP()\n",
    "    \n",
    "\n",
    "    Should raise an error if an invalid input is passed to add_layer\n",
    "    >>> mlp = MLP()\n",
    "    >>> mlp.add_layer(np.array([]))\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    ValueError: layer must be of at least length 1\n",
    "\n",
    "    Should add the input layer and a hidden layer\n",
    "    >>> mlp = MLP(hidden_layer_size=(4,2))\n",
    "    >>> mlp.add_layer(np.array([Perceptron() for _ in range(2)]))\n",
    "    >>> mlp.layers\n",
    "    array([array([Perceptron(), Perceptron()], dtype=object),\n",
    "           array([[Perceptron(), Perceptron()],\n",
    "                  [Perceptron(), Perceptron()],\n",
    "                  [Perceptron(), Perceptron()],\n",
    "                  [Perceptron(), Perceptron()]], dtype=object)], dtype=object)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learn_rate=0.001, n_iters=100, hidden_layer_size=(50,1)):\n",
    "        self._validate_input_params(learn_rate, n_iters, hidden_layer_size)\n",
    "        self.learn_rate = learn_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{type(self).__name__}()\"\n",
    "\n",
    "    def _validate_input_params(self, learn_rate, n_iters, hidden_layer_size):\n",
    "        if not isinstance(n_iters, int) or n_iters < 1:\n",
    "            raise ValueError(\"n_iters must be an integer and a natural number\")\n",
    "        if not isinstance(learn_rate, (int, float)) or learn_rate <= 0:\n",
    "            raise ValueError(\"learn_rate must be a float or int greater than 0\")\n",
    "        n, m = hidden_layer_size\n",
    "        if not isinstance(n, int) or n < 1:\n",
    "            raise ValueError(\"hidden_layer_size must contain natural numbers of type int\")\n",
    "        if not isinstance(m, int) or m < 1:\n",
    "            raise ValueError(\"hidden_layer_size must contain natural numbers of type int\")\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        if len(layer) == 0:\n",
    "            raise ValueError(\"layer must be of at least length 1\")\n",
    "        if hasattr(self, 'layers'):\n",
    "            ls = self.layers.tolist()\n",
    "            ls.append(layer)\n",
    "            self.layers = np.array(ls, dtype=object)\n",
    "        else:\n",
    "            n, m = self.hidden_layer_size\n",
    "            ls = [[Perceptron() for _ in range(n)] for _ in range(m) ]\n",
    "            ls.insert(0, layer.tolist())\n",
    "            self.layers = np.array(ls)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # for e/a perceptron in each layer call fit\n",
    "        # need to forward_propegate the weights and bias from the perceptron, to the next... I think \n",
    "\n",
    "        for P in self.layers:\n",
    "            for p in P:\n",
    "                p.fit(X, y)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def fit_predict(self):\n",
    "        pass\n",
    "\n",
    "df = read_data_return_dataframe(\"./wildfires.txt\")\n",
    "df_train, df_test = split_df_to_train_test_dfs(df)\n",
    "y_train, X_train = split_df_labels_attributes(df_train)\n",
    "y_train = normalise_outputs(y_train)\n",
    "    \n",
    "mlp = MLP(learn_rate=.02, n_iters= 500)\n",
    "mlp.add_layer(np.array([Perceptron() for _ in range(4)]))\n",
    "mlp.fit(X_train, y_train)\n",
    "# # mlp.add_layer(np.array([]))\n",
    "\n",
    "# print(mlp.layers)\n",
    "# mlp.add_layer([[1,2,3]])\n",
    "# mlp.add_layer([[4,5,3]])\n",
    "# mlp.add_layer([[6,7,3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    MLP(n_iters=\"whoops\")\n",
      "Expecting:\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: n_iters must be an integer and a natural number\n",
      "ok\n",
      "Trying:\n",
      "    MLP(hidden_layer_size=(0,0))\n",
      "Expecting:\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: hidden_layer_size must contain natural numbers of type int\n",
      "ok\n",
      "Trying:\n",
      "    MLP(learn_rate=0)\n",
      "Expecting:\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: learn_rate must be a float or int greater than 0\n",
      "ok\n",
      "Trying:\n",
      "    mlp = MLP()\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    mlp\n",
      "Expecting:\n",
      "    MLP()\n",
      "ok\n",
      "Trying:\n",
      "    mlp = MLP()\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    mlp.add_layer(np.array([]))\n",
      "Expecting:\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: layer must be of at least length 1\n",
      "ok\n",
      "Trying:\n",
      "    mlp = MLP(hidden_layer_size=(4,2))\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    mlp.add_layer(np.array([Perceptron() for _ in range(2)]))\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    mlp.layers\n",
      "Expecting:\n",
      "    array([array([Perceptron(), Perceptron()], dtype=object),\n",
      "           array([[Perceptron(), Perceptron()],\n",
      "                  [Perceptron(), Perceptron()],\n",
      "                  [Perceptron(), Perceptron()],\n",
      "                  [Perceptron(), Perceptron()]], dtype=object)], dtype=object)\n",
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_24124\\1907392274.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.layers = np.array([layer, np.full(self.hidden_layer_size, Perceptron())])\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(MLP, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c69e896b73d80df03b10aefe902562c227bdab9e6e1527e46fe261fc763f811"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
