{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT5170: Principles of ML - Assignment 2\n",
    "### Course code: 1MAO3\n",
    "#### Participants (name: id): (Daniel Verdejo: 22240224, Thomas Cronin: 22239435)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration and Clean-up\n",
    "##### Tom's Work\n",
    "A helper function was created to load the dataset into a pandas DataFrame; this function ‘read_data_return_dataframe’ takes a string containing the path to a txt file. It reads the dataset via pandas.read() and returns the DataFrame containing the data.\n",
    "\n",
    "Exploring the data showed the ranges and statistical data, while being quite distinct from each other and all in a usable state, would require normalisation. The ‘fire’ column, however, did not look correct; it contained eight unique entries for what should have been two individual entries, ‘yes’ or ‘no’. It seemed like the data was being read with questionable spaces giving them, as far as panda DataFrames are concerned, unique entries. To replace the entries, a convert_label function was implemented to replace a specified binary value with a different binary value. This allowed the change from ‘yes/no’ to ‘1/0’ while also replacing duplicate entries.\n",
    "\n",
    "The dataset is imbalanced, slightly skewed to the positive case ‘yes’. There are 97 ‘no’ and 107 ‘yes’.\n",
    "\n",
    "To normalise the data, the helper function Normalize was implemented to normalise all the data to 0-1 ranges. It takes a DataFrame and a list of features, returning a DataFrame of the normalised value of each entry in each feature column.\n",
    "\n",
    "##### Daniels work\n",
    "\n",
    "Splitting data into training_data, testing_data, training_labels, testing_labels is done using some simple dataframe functionality. First by splitting the dataset into 2 fractions (train and test) typically around 80 - 90% train with the remainder being test. These are then further split down into their labels and features / attributes using  `df.iloc` with some simple list comprehension taking all rows and segregating by column Finally to normalise the labels another simple list comprehension is used to map values 1 to every 'yes' found, and 0 to every 'no' found in the labels. \n",
    "\n",
    "### Algorithms\n",
    "#### Perceptron Algorithm\n",
    "##### Tom's Work\n",
    "<u>Design Decisions:</u>\n",
    "* I choose not to include a bias modification for the perceptron as it would not be needed to hopefully get a high accuracy on this dataset, based on the data exploration.\n",
    "* The perceptron initiates the weights if not specified between random values of (-0.5, 0.5) after researching the algorithm this was deemed a suitable weight range. It is also possible to pass predefined weights to the nueron.\n",
    "* For the sigmoid activation function it was decided that a summed weights=0, which in sigmoid would return a 0.5 would still be a positive case and would activate the perceptron with a 1.\n",
    "\n",
    "<u>Algorithm Design</u>\n",
    "   Pseudo Code:\n",
    "\n",
    "        ThresholdLogicUnit()\n",
    "            Learning Rate = Input()\n",
    "            Input Weights = Initialised Randomly or Input\n",
    "            Activation Functions = Heaviside or Input\n",
    "          \n",
    "            fit()\n",
    "                Training Set = Input()\n",
    "                Labels = Input() or None\n",
    "                Learning Iterations = Input or 200\n",
    "\n",
    "                for each iteration:\n",
    "                    predict value on inputs\n",
    "                    compare predictions with results\n",
    "                    if prediction is wrong update weights:\n",
    "                        new weights = weighs -  value of learning rate multiplied by (prediction - label) multiplied by input\n",
    "            predict()\n",
    "                samples = Input\n",
    "                for each sample predict a score\n",
    "                return a list of the predictions\n",
    "\n",
    "The Perceptron Algorithm:\n",
    "    \n",
    "The Threshold Logic Unit takes in n_dimentional data and there corresponding 1_dimentional array of labels. It requires a learning rate, which is used to tune the changing of the weights values. After getting the data and labels, the weights for the amount of features in a sample (the input shape) is initialised  randomly between values of -0.5,+0.5 (note: these can be set aswell). The Threshold Logic Unit then loops for the amount of iterations it should run and trains on each sample. During the training process it will predict a score using the specified activation function. If the score is not equal to the coresponding label, the weights are updated by subtracting the current weights with the learning rate * error * the input. Once the TLU has completed its training, we can then pass it an unseen sample and get a prediction.\n",
    "\n",
    "#### Multi Layer Perceptron Algorithm\n",
    "\n",
    "For the comparrison of our implementation of the MultiLayer Perceptron the decision was made to use MLPClassifier found in sklearn.neural_network package. It can be seen from the scores above that sklearns implementation is better at classifying the data over our implementation of the MultiLayer Perceptron(MLP). The reasoning behind this is likely due to some design decisions which we're made on the perceptron, which will be discussed shortly.\n",
    "\n",
    "First lets review the design of the MLP neural network(NN) model.\n",
    "\n",
    "The MLP is built up of `n` amount of layers. When we construct the object we initialise its layers and weights to empty lists. From there, every time the `add_layer` function is called, the weights are added for the given layer. A layer consists of 'm' number of neurons, our Threshold Logic Unit(TLU) perceptrons. The layer itself manages its neurons with respect to how its `fit, predict, update, and initialise` functions are called. The layer passes arguments down to each of its child neurons including the `learning_rate` and `activation`. The activation function defines what the neurons outputs will be, whether they are `0` and `1` as you would get from a sigmoid or heaviside function, or a value ranging from -.5 to greater than 0 that we get from the ReLU activation function.\n",
    "\n",
    "Reverting to the MLP NN, some functions are defined on the class as: forward_propegation, back_propegation, train, and predict.\n",
    "\n",
    "1. The `forward_propegation` function is the process of forwarding the output of the previous hidden layer into the layer which follows. The input -> first hidden layer is unique as it receives `X`, our dataset in its \"raw\" form (after normalisation, splitting and so on.). The first hidden layer in our example is initialised using the ReLU function for its child neurons. The output of the layer is propegated to the next layer in the network, which will pass its output to the layer which follows it until we come to the output layer, the end of the NN. The output layer returns a list of classifications that is the size of `n` number of samples (e.g. 50 samples are fed to the NN, 50 classifications are returned from the NN).\n",
    "2. The `back_propegation` function is the process of feeding back the error and delta in our outputs relative the the labels for every layer in the network. The error is gathered by comparing the output `Z` of the `forward_propegation` function relative to the labels `Y`. The delta is then calculated by multiplying the error of the output by the derivative of sigmoid `Z`.  With this information the NN can update the weights for each layer based off the dot products of the individual error and delta for each layer. The update value from the dot product of the transposed `X`, `d` (delta per layer) is passed to the layer which feeds the update into its neurons (perceptrons). Typically the bias would also be calculated, but the decision was made to exclude the bias from the perceptron, additionally removing the need to add or process biases in the `forward_propegation` and `back_propegate` functions.\n",
    "3. The `train` function simply calls the `forward_propegation` and `back_propegation` functions `n` times. It also has the ability to show metrics as the iterations go by. Finally it returns the outputs of the `forward_propegation` function.\n",
    "4. The `predict` function similar to the `train` function calls `forward_propegation` but only once and returns its output.\n",
    " \n",
    "\n",
    "### Test's and Result's\n",
    "#### Perceptron\n",
    "##### Tom's Work\n",
    "<u>The Perceptron Comparisons:</u>\n",
    "The Perceptron I chose to compare to as the refrence Perceptron is SK_learns Perceptron Class.\n",
    "\n",
    "Testing the Sk_Learn Perceptron:\n",
    "* When exploring the Perceptron against the training set, I was curious to see what the learning iterartions was as in the current version of sk_learn you cannot manually set it, however it is possible to see the number of learning iterations it completes after fitting, this can be done by accessing the Perceptron().n_iter_ variable. I ran it several time with different sets of data and it never went past 20 iterations. Inclusive of the Traing sets for wildfires.txt\n",
    "\n",
    "The overall tests were run on 5 versions of the wildfires data with a split ratio of:\n",
    "\n",
    "The Accuracy, Recall, Precison and f1_score was then calculated for each:\n",
    "\n",
    "    | Train Percent | Test Percent|| Accuracy | Recall| Precison | f1_score |\n",
    "    |---------------|-------------||----------|-------|----------|----------|\n",
    "    |90%            |10%          ||0.3414    |0.3478 | 0.4      |0.372     |\n",
    "    |80%            |20%          ||0.4634    |0.5416 |0.5416    | 0.5416   |\n",
    "    |70%            |30%          ||0.5609    |0.625, |0.6896    |0.6896    |\n",
    "    |60%            |40%          ||0.4878    |0.5769 |0.6       |0.5882    |\n",
    "    |50%            |50%          ||0.4878    |0.5384 |0.3181    |.3999     |\n",
    "\n",
    "The Singular Perceptron didn't do that well on the data set, as the test to train ratio increased it started to become better at genearlising, with the best train test split being 7:3. Overall however the single perceptron's accuracy was about as good as flippng a coin each time and as the dataset was not quite but almost balanced this would sugest the model is almost just guessing yes or no each time.\n",
    "\n",
    "Testing the Threshold Logic Unit Perceptron:\n",
    "* In an attempt to keep the testing of both models realtivly fair and even, I set the learning rate of the TLU to be that of the SK_Learn Perceptron and reduced the iterations to be no more than 20 iterations per training. \n",
    "\n",
    "The overall tests were run on 5 versions of the wildfires data with a split ratio of:\n",
    "\n",
    "The Accuracy, Recall, Precison and f1_score was then calculated for each:\n",
    "\n",
    "    | Train Percent | Test Percent|| Accuracy | Recall| Precison | f1_score |\n",
    "    |---------------|-------------||----------|-------|----------|----------|\n",
    "    |90%            |10%          ||0.658     |0.769  | 0.869    |0.816     |\n",
    "    |80%            |20%          ||0.756     |0.730  |0.863     |0.791     |\n",
    "    |70%            |30%          ||0.780     |0.791  |0.826     |0.808     |\n",
    "    |60%            |40%          ||0.658     |0.714  |0.652     |0.681     |\n",
    "    |50%            |50%          ||0.682     |0.68   |0.772     |0.723     |\n",
    "\n",
    "The TLU performs decently on all variations of the train to test split, the scores never did better than 82% f1_score when constircted by the paramaters applied the the SK_Learn Perceptron. By increasing the learning iterations of the TLU the f1_score hit 95% and the accuracy was around the same. \n",
    "\n",
    "<u>Comparing Averaged Scores</u>\n",
    "\n",
    "    | Test Percent      ||Avg: Accuracy |Avg: Recall|Avg: Precison |Avg: f1_score |\n",
    "    |-------------------||--------------|-----------|--------------|--------------|\n",
    "    |SK_Learn Perceptron||0.468         |0.525     |0.525         |0.518         |\n",
    "    |TLU Perceptron     ||0.731         |0.737      |0.796         |0.764         |\n",
    "\n",
    "In most cases the TLU perceptron outpreformed the SK_learn Perceptron by about 30%. If Sk_Learns Perceptron had the ability to modify the Learning Iterations I feel it would have most likely gotten a better score.\n",
    "\n",
    "#### MLP Comparissons\n",
    "##### Daniels work\n",
    "<u>The MLP Comparisons:</u>\n",
    "The chosen MLP comparisson was sklearn.neural_network: MLPClassifier\n",
    "\n",
    "Overall the data suggests that the implementation done here performed worse on average than the sklearn implentation. This could be from a number of factors but potentially a big one being the omission of biases in the neural network. Some of the accuracies show very poor performances akin to guessing. I had noted that it performs better when fed large batches but the data below is from batches of 15 samples\n",
    "\n",
    "The overall tests were run on 5 versions of the wildfires data with a split ratio of:\n",
    "\n",
    "The Accuracy, Recall, Precison and f1_score was then calculated for each:\n",
    "\n",
    "Our implentations\n",
    "\n",
    "         Ratio  Accuracy, Precision, Recall, F1_score\n",
    "    MLP: 0.3   0.4666,    0.3846,    1.0,    0.5555\n",
    "    MLP: 0.27  0.2666,    0.125,     0.2,    0.153\n",
    "    MLP: 0.35  0.4666,    0.54545,   0.6666, 0.6\n",
    "    MLP: 0.1   0.4,       0.3333,    0.285,  0.3076\n",
    "    MLP: 0.2   0.6666,    0.2,       0.5,    0.2857\n",
    "\n",
    "Sklearn implementation\n",
    "\n",
    "         Ratio  Accuracy, Precision, Recall, F1_score\n",
    "    MLP: 0.3    0.73333,  0.6666,    0.4,    0.5\n",
    "    MLP: 0.27   0.4,      0.333333,  0.8,    0.4705\n",
    "    MLP: 0.35   0.8,      0.875,     0.7777, 0.8235\n",
    "    MLP: 0.1    0.8,      0.7,       1.0,    0.8235\n",
    "    MLP: 0.2    0.7333,   0.3333,    1.0,    0.5\n",
    "\n",
    "Averages:\n",
    "\n",
    "     Test Percent Avg: Accuracy Avg: RecallAvg: Precison Avg: f1_score \n",
    "    SK_Learn Perceptron, 0.468         ,0.525     ,0.525         ,0.518       \n",
    "    TLU Perceptron     , 0.731         ,0.737     ,0.796         ,0.764     \n",
    "\n",
    "Overall the NN's performance shows ~20 - 30% less accurate when compared to sklearns implementation. Additionally the recall, f1 score and precision show values of ~10 - 30 points lower on average when compared against sklearns implementation. Conclusion more time and work is needed to improve the MLP NN.\n",
    "\n",
    "### Conclusion and observations\n",
    "##### Tom and Danny's Conclusions and observaitons\n",
    "\n",
    "***Observations and conclusions on the Perceptron***\n",
    "While the TLU unit outperformed the SK_Learn Perceptron most of the time, it was incosistent. Sometimes it under performed significantly, whereas the SK_Learn while less accurate overall was consistent in its accuracy. I believe that had I implemented the Biases to the Perceptron its accuracy scores would be more consistent and inline with the SK_Learns Perceptron, however for this dataset as it outperformed on the Test set I am overall happy with its implementaion. Modifying the perceptron to accept activation functions rather than store preset activations functions would have also benefited its design for futrure implementaion and use within the MLP.\n",
    "***Observations and conclusions on the MLP implementaion***\n",
    "As briefly mentioned in the `back_propegation` function description, when comparing the local MLP implementation against the third party implementation, it is a possibility that while not including biases on a perceptron level has a negligable effect, it is likely that the omission on a network level could lead to poorer performance. _While this decision simplified the implementation on all levels (Perceptron, Layer, MLP), some more time and experimentation with these biases added in would show whether the omission of the biases was the right choice_. \n",
    "\n",
    "Overall the learning outcome from this assignment was beneficial and both Tom and I are happy with what we have learned, where we can improve in the future, but most importantly, we now have our own reference of how to build a neural net from scratch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seaborn import pairplot\n",
    "from Utils import *\n",
    "from Metrics import *\n",
    "\n",
    "from ThresholdLogicUnit import ThresholdLogicUnit\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 10)\n",
      "--------------------\n",
      "Index(['fire', 'year', 'temp', 'humidity', 'rainfall', 'drought_code',\n",
      "       'buildup_index', 'day', 'month', 'wind_speed'],\n",
      "      dtype='object')\n",
      "--------------------\n",
      "fire              object\n",
      "year               int64\n",
      "temp               int64\n",
      "humidity           int64\n",
      "rainfall         float64\n",
      "drought_code     float64\n",
      "buildup_index    float64\n",
      "day                int64\n",
      "month              int64\n",
      "wind_speed         int64\n",
      "dtype: object\n",
      "--------------------\n",
      "          fire         year        temp    humidity    rainfall  drought_code  \\\n",
      "count      204   204.000000  204.000000  204.000000  204.000000    204.000000   \n",
      "unique       8          NaN         NaN         NaN         NaN           NaN   \n",
      "top     yes             NaN         NaN         NaN         NaN           NaN   \n",
      "freq       101          NaN         NaN         NaN         NaN           NaN   \n",
      "mean       NaN  2011.975490   31.906863   62.279412    0.823529     48.537647   \n",
      "std        NaN     3.320987    3.814175   15.209388    2.117959     49.133366   \n",
      "min        NaN  2007.000000   22.000000   21.000000    0.000000      7.180000   \n",
      "25%        NaN  2009.000000   29.000000   52.750000    0.000000     10.627500   \n",
      "50%        NaN  2012.000000   32.000000   63.000000    0.000000     30.550000   \n",
      "75%        NaN  2015.000000   35.000000   74.250000    0.600000     62.367500   \n",
      "max        NaN  2017.000000   43.000000   92.000000   16.800000    221.350000   \n",
      "\n",
      "        buildup_index         day       month  wind_speed  \n",
      "count      204.000000  204.000000  204.000000  204.000000  \n",
      "unique            NaN         NaN         NaN         NaN  \n",
      "top               NaN         NaN         NaN         NaN  \n",
      "freq              NaN         NaN         NaN         NaN  \n",
      "mean        16.542304   15.691176    7.553922   16.446078  \n",
      "std         14.634994    8.907722    1.196067    3.098074  \n",
      "min          1.320000    1.000000    6.000000    6.000000  \n",
      "25%          6.067500    8.000000    6.000000   14.750000  \n",
      "50%         11.535000   15.500000    8.000000   16.000000  \n",
      "75%         22.665000   24.000000    9.000000   18.000000  \n",
      "max         68.270000   31.000000    9.000000   30.000000  \n"
     ]
    }
   ],
   "source": [
    "# Tom Cronin\n",
    "loaded_data_frame = read_data_return_dataframe(\"../wildfires.txt\") # Loads The wildfire Dataset\n",
    "wildfires_df = loaded_data_frame.copy() # copys the data so we don't mess with the original dataset\n",
    "\n",
    "print(wildfires_df.shape) # gets the dimensions of the dataframe\n",
    "print(\"-\" * 20)\n",
    "print(wildfires_df.columns) # gets the features of the columns\n",
    "print(\"-\" * 20)\n",
    "print(wildfires_df.dtypes) # returns the datatypes\n",
    "print(\"-\" * 20)\n",
    "print(wildfires_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error in fire label?\n",
    "It seems that the fire label is a mix of 'no' and 'yes' however there are different spaces causing the dataframe to think there are multiple entries, 8 instead of 2. Let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fire</th>\n",
       "      <th>year</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>drought_code</th>\n",
       "      <th>buildup_index</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>204</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>YES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2011.975490</td>\n",
       "      <td>31.906863</td>\n",
       "      <td>62.279412</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>48.537647</td>\n",
       "      <td>16.542304</td>\n",
       "      <td>15.691176</td>\n",
       "      <td>7.553922</td>\n",
       "      <td>16.446078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.320987</td>\n",
       "      <td>3.814175</td>\n",
       "      <td>15.209388</td>\n",
       "      <td>2.117959</td>\n",
       "      <td>49.133366</td>\n",
       "      <td>14.634994</td>\n",
       "      <td>8.907722</td>\n",
       "      <td>1.196067</td>\n",
       "      <td>3.098074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.180000</td>\n",
       "      <td>1.320000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>52.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.627500</td>\n",
       "      <td>6.067500</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.550000</td>\n",
       "      <td>11.535000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>74.250000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>62.367500</td>\n",
       "      <td>22.665000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>221.350000</td>\n",
       "      <td>68.270000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fire         year        temp    humidity    rainfall  drought_code  \\\n",
       "count   204   204.000000  204.000000  204.000000  204.000000    204.000000   \n",
       "unique    2          NaN         NaN         NaN         NaN           NaN   \n",
       "top     YES          NaN         NaN         NaN         NaN           NaN   \n",
       "freq    107          NaN         NaN         NaN         NaN           NaN   \n",
       "mean    NaN  2011.975490   31.906863   62.279412    0.823529     48.537647   \n",
       "std     NaN     3.320987    3.814175   15.209388    2.117959     49.133366   \n",
       "min     NaN  2007.000000   22.000000   21.000000    0.000000      7.180000   \n",
       "25%     NaN  2009.000000   29.000000   52.750000    0.000000     10.627500   \n",
       "50%     NaN  2012.000000   32.000000   63.000000    0.000000     30.550000   \n",
       "75%     NaN  2015.000000   35.000000   74.250000    0.600000     62.367500   \n",
       "max     NaN  2017.000000   43.000000   92.000000   16.800000    221.350000   \n",
       "\n",
       "        buildup_index         day       month  wind_speed  \n",
       "count      204.000000  204.000000  204.000000  204.000000  \n",
       "unique            NaN         NaN         NaN         NaN  \n",
       "top               NaN         NaN         NaN         NaN  \n",
       "freq              NaN         NaN         NaN         NaN  \n",
       "mean        16.542304   15.691176    7.553922   16.446078  \n",
       "std         14.634994    8.907722    1.196067    3.098074  \n",
       "min          1.320000    1.000000    6.000000    6.000000  \n",
       "25%          6.067500    8.000000    6.000000   14.750000  \n",
       "50%         11.535000   15.500000    8.000000   16.000000  \n",
       "75%         22.665000   24.000000    9.000000   18.000000  \n",
       "max         68.270000   31.000000    9.000000   30.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tom\n",
    "ndarray = wildfires_df['fire'].copy() # gets the fire column\n",
    "for index in range(len(ndarray)):   # loops through each entry\n",
    "    if 'no' in ndarray[index].lower():\n",
    "        ndarray[index] = \"NO\"   # sets label to No\n",
    "    elif 'yes' in ndarray[index].lower():\n",
    "        ndarray[index] = \"YES\" # sets label to yes\n",
    "wildfires_df['fire'] = ndarray\n",
    "labels_copy_df = wildfires_df['fire'].copy()\n",
    "wildfires_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fire is now a binary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'year'}>,\n",
       "        <AxesSubplot:title={'center':'temp'}>,\n",
       "        <AxesSubplot:title={'center':'humidity'}>],\n",
       "       [<AxesSubplot:title={'center':'rainfall'}>,\n",
       "        <AxesSubplot:title={'center':'drought_code'}>,\n",
       "        <AxesSubplot:title={'center':'buildup_index'}>],\n",
       "       [<AxesSubplot:title={'center':'day'}>,\n",
       "        <AxesSubplot:title={'center':'month'}>,\n",
       "        <AxesSubplot:title={'center':'wind_speed'}>]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tom Cronin\n",
    "# Lets Look at the data graphically\n",
    "wildfires_df.hist(figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Algorithm\n",
    "For sake of clarity and shortness of this file, the perceptron was implemented as a ThresholdLogicUnit in a seperate file. Can be found in the zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_preds(prediction, labels):\n",
    "    scores = []\n",
    "    scores.append(accuracy(prediction, labels))\n",
    "    scores.append(precision(prediction, labels))\n",
    "    scores.append(recall(prediction, labels))\n",
    "    scores.append(f1_score(prediction, labels))\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildfires = read_data_return_dataframe(\"../wildfires.txt\")\n",
    "wildfires_copy = wildfires.copy()\n",
    "test_ratios = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "my_perceptron = []\n",
    "sk_learn_pereptron = []\n",
    "\n",
    "for ratio in test_ratios:\n",
    "    features = ['year', 'temp', 'humidity', 'rainfall', 'drought_code', 'buildup_index', 'day', 'month', 'wind_speed']\n",
    "    X_train, X_test, y_train, y_test = split_df_to_train_test_dfs(wildfires_copy, test_set_size=.2,\n",
    "                                                        random_state=42)\n",
    "      \n",
    "    X_train = X_train[features].values  # returns a numpy NdArray of the features\n",
    "    X_test = X_test[features].values  # returns a numpy NdArray of the features\n",
    "    X_train = Normalize(X_train, features)\n",
    "    X_test = Normalize(X_test, features)\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    y_train = np.asarray(y_train).flatten()\n",
    "    y_test = np.asarray(y_test).flatten()\n",
    "    y_train = np.asarray([1 if 'yes' in y else 0 for y in y_train])\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_test = np.asarray([1 if 'yes' in y else 0 for y in y_test])\n",
    "\n",
    "\n",
    "    perceptron = ThresholdLogicUnit(learning_rate=0.001, activation_function='sigmoid')\n",
    "    perceptron.fit(X_train,y_train, learning_iterations=20)\n",
    "\n",
    "    pred_train = perceptron.predict(np.asarray(X_train))\n",
    "    predictions = perceptron.predict(np.asarray(X_test))\n",
    "    \n",
    "    my_perceptron.append(score_preds(predictions, y_test))\n",
    "    \n",
    "    y_train= y_train.astype('int')\n",
    "    y_test= y_train.astype('int')\n",
    "    sklp = Perceptron()\n",
    "\n",
    "    y_train= y_train.astype('int')\n",
    "    y_test= y_train.astype('int')\n",
    "    sklp.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    pred_train = sklp.predict(X_train)\n",
    "    predictions = sklp.predict(X_test)\n",
    "    sk_learn_pereptron.append(score_preds(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare my Perceptron with SKL Perceptron\n",
      "Test Ratio:  0.1\n",
      "Accuracy, Precision, Recall, F1_score\n",
      "TLU:  [0.5609756097560976, 0.5416666666666666, 0.65, 0.5909090909090908]\n",
      "SKL:  [0.6097560975609756, 0.5555555555555556, 0.7894736842105263, 0.6521739130434783]\n",
      "Test Ratio:  0.2\n",
      "Accuracy, Precision, Recall, F1_score\n",
      "TLU:  [0.6585365853658537, 0.7142857142857143, 0.5, 0.588235294117647]\n",
      "SKL:  [0.3902439024390244, 0.4090909090909091, 0.42857142857142855, 0.4186046511627907]\n",
      "Test Ratio:  0.3\n",
      "Accuracy, Precision, Recall, F1_score\n",
      "TLU:  [0.6341463414634146, 0.6551724137931034, 0.7916666666666666, 0.7169811320754716]\n",
      "SKL:  [0.5365853658536586, 0.6842105263157895, 0.5, 0.5777777777777778]\n",
      "Test Ratio:  0.4\n",
      "Accuracy, Precision, Recall, F1_score\n",
      "TLU:  [0.8292682926829268, 0.9047619047619048, 0.7916666666666666, 0.8444444444444444]\n",
      "SKL:  [0.36585365853658536, 0.391304347826087, 0.42857142857142855, 0.4090909090909091]\n",
      "Test Ratio:  0.5\n",
      "Accuracy, Precision, Recall, F1_score\n",
      "TLU:  [0.7073170731707317, 0.8888888888888888, 0.6153846153846154, 0.7272727272727274]\n",
      "SKL:  [0.4878048780487805, 0.45, 0.47368421052631576, 0.46153846153846156]\n",
      "\n",
      "Average Scores tlu [0.67804878 0.74095512 0.66974359 0.69356854]\n",
      "Average Scores skl [0.47804878 0.49803227 0.52406015 0.50383714]\n"
     ]
    }
   ],
   "source": [
    "print(\"Compare my Perceptron with SKL Perceptron\")\n",
    "\n",
    "for index in  range(len(test_ratios)):\n",
    "    print(\"Test Ratio: \", test_ratios[index])\n",
    "    print(\"Accuracy, Precision, Recall, F1_score\")\n",
    "    print(\"TLU: \", my_perceptron[index])\n",
    "    print(\"SKL: \", sk_learn_pereptron[index])\n",
    "print() \n",
    "print('Average Scores tlu',  np.average(np.array(my_perceptron), axis=0))\n",
    "print('Average Scores skl',  np.average(np.array(sk_learn_pereptron), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 / 200 ============= Acc: 0.4666666666666667\n",
      "iteration: 25 / 200 ============= Acc: 0.4666666666666667\n",
      "iteration: 50 / 200 ============= Acc: 0.4666666666666667\n",
      "iteration: 75 / 200 ============= Acc: 0.4666666666666667\n",
      "iteration: 100 / 200 ============= Acc: 0.4666666666666667\n",
      "iteration: 125 / 200 ============= Acc: 0.4666666666666667\n",
      "iteration: 150 / 200 ============= Acc: 0.4666666666666667\n",
      "iteration: 175 / 200 ============= Acc: 0.4666666666666667\n",
      "Prediction: [[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 / 200 ============= Acc: 0.8\n",
      "iteration: 25 / 200 ============= Acc: 0.8\n",
      "iteration: 50 / 200 ============= Acc: 0.8\n",
      "iteration: 75 / 200 ============= Acc: 0.8\n",
      "iteration: 100 / 200 ============= Acc: 0.8\n",
      "iteration: 125 / 200 ============= Acc: 0.8\n",
      "iteration: 150 / 200 ============= Acc: 0.8\n",
      "iteration: 175 / 200 ============= Acc: 0.8\n",
      "Prediction: [[0 1 0 1 1 0 1 1 0 0 1 0 1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 / 200 ============= Acc: 0.5333333333333333\n",
      "iteration: 25 / 200 ============= Acc: 0.5333333333333333\n",
      "iteration: 50 / 200 ============= Acc: 0.5333333333333333\n",
      "iteration: 75 / 200 ============= Acc: 0.5333333333333333\n",
      "iteration: 100 / 200 ============= Acc: 0.5333333333333333\n",
      "iteration: 125 / 200 ============= Acc: 0.5333333333333333\n",
      "iteration: 150 / 200 ============= Acc: 0.5333333333333333\n",
      "iteration: 175 / 200 ============= Acc: 0.5333333333333333\n",
      "Prediction: [[1 0 1 0 0 1 1 1 1 1 1 0 1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 / 200 ============= Acc: 0.3333333333333333\n",
      "iteration: 25 / 200 ============= Acc: 0.3333333333333333\n",
      "iteration: 50 / 200 ============= Acc: 0.3333333333333333\n",
      "iteration: 75 / 200 ============= Acc: 0.3333333333333333\n",
      "iteration: 100 / 200 ============= Acc: 0.3333333333333333\n",
      "iteration: 125 / 200 ============= Acc: 0.3333333333333333\n",
      "iteration: 150 / 200 ============= Acc: 0.3333333333333333\n",
      "iteration: 175 / 200 ============= Acc: 0.3333333333333333\n",
      "Prediction: [[0 0 0 0 0 0 0 1 1 0 1 0 1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 / 200 ============= Acc: 0.6\n",
      "iteration: 25 / 200 ============= Acc: 0.6\n",
      "iteration: 50 / 200 ============= Acc: 0.6\n",
      "iteration: 75 / 200 ============= Acc: 0.6\n",
      "iteration: 100 / 200 ============= Acc: 0.6\n",
      "iteration: 125 / 200 ============= Acc: 0.6\n",
      "iteration: 150 / 200 ============= Acc: 0.6\n",
      "iteration: 175 / 200 ============= Acc: 0.6\n",
      "Prediction: [[1 0 1 0 1 1 0 0 1 0 1 0 1 0 0]]\n",
      "Compare our MLP with SKL MLP\n",
      "Test Ratio:  0.3\n",
      "Accuracy, Precision, Recall, F1_score\n",
      "MLP:  [0.4666666666666667, 0.38461538461538464, 1.0, 0.5555555555555556]\n",
      "SKL:  [0.7333333333333333, 0.6666666666666666, 0.4, 0.5]\n",
      "Test Ratio:  0.27\n",
      "Accuracy, Precision, Recall, F1_score\n",
      "MLP:  [0.26666666666666666, 0.125, 0.2, 0.15384615384615385]\n",
      "SKL:  [0.4, 0.3333333333333333, 0.8, 0.47058823529411764]\n",
      "Test Ratio:  0.35\n",
      "Accuracy, Precision, Recall, F1_score\n",
      "MLP:  [0.4666666666666667, 0.5454545454545454, 0.6666666666666666, 0.6]\n",
      "SKL:  [0.8, 0.875, 0.7777777777777778, 0.823529411764706]\n",
      "Test Ratio:  0.1\n",
      "Accuracy, Precision, Recall, F1_score\n",
      "MLP:  [0.4, 0.3333333333333333, 0.2857142857142857, 0.30769230769230765]\n",
      "SKL:  [0.8, 0.7, 1.0, 0.8235294117647058]\n",
      "Test Ratio:  0.2\n",
      "Accuracy, Precision, Recall, F1_score\n",
      "MLP:  [0.6666666666666666, 0.2, 0.5, 0.28571428571428575]\n",
      "SKL:  [0.7333333333333333, 0.3333333333333333, 1.0, 0.5]\n",
      "\n",
      "Average Scores MLP [0.45333333 0.31768065 0.53047619 0.38056166]\n",
      "Average Scores skl [0.69333333 0.58166667 0.79555556 0.62352941]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from MLP import MLP\n",
    "from Utils import *\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from Metrics import *\n",
    "import random\n",
    "\n",
    "random.seed(2060)\n",
    "\n",
    "data = read_data_return_dataframe(\"../wildfires.txt\")\n",
    "# Copy to be used for the rest of the assignment\n",
    "wildfires_copy = data.copy()\n",
    "# wildfires_copy = convert_label(wildfires,'fire',['no', 'yes'],[0, 1])\n",
    "\n",
    "\n",
    "features = ['year', 'temp', 'humidity', 'rainfall', 'drought_code', 'buildup_index', 'day', 'month', 'wind_speed']\n",
    "mlp_vals = []\n",
    "sk_vals = []\n",
    "for ratio in [.3, .27, .35, .1, .20]:\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_df_to_train_test_dfs(wildfires_copy, test_set_size=ratio,\n",
    "                                                        random_state=42)\n",
    "    X_train = X_train[features].values  # returns a numpy NdArray of the features\n",
    "    X_test = X_test[features].values  # returns a numpy NdArray of the features\n",
    "    X_train = Normalize(X_train, features)\n",
    "    X_test = Normalize(X_test, features)\n",
    "\n",
    "    X_train = np.asarray(X_train)[0:15]\n",
    "    y_train = np.asarray(y_train).flatten()\n",
    "    y_test = np.asarray(y_test).flatten()\n",
    "\n",
    "    y_train = np.asarray([1 if 'yes' in y else 0 for y in y_train])[0:15]\n",
    "    X_test = np.asarray(X_test)[0:15]\n",
    "    y_test = np.asarray([1 if 'yes' in y else 0 for y in y_test])[0:15]\n",
    "\n",
    "    m, n = X_train.shape\n",
    "    mlp = MLP()\n",
    "    mlp.add_layer(output_size = m, activation='relu', input_size=n) # Add a layer of 9 inputs and 32 outputs\n",
    "    mlp.add_layer(output_size = 1, activation='sigmoid', input_size= m) # add output layer with 32 inputs and 1 output\n",
    "    mlp.train(X=X_train, Y=y_train, iters=200, show_metrics=True)\n",
    "\n",
    "    clf = MLPClassifier(random_state=1, max_iter=200).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    p1 = mlp.predict(X_test)[0]\n",
    "    p2 = clf.predict(X_test)\n",
    "    \n",
    "    mlp_vals.append(score_preds(p1, y_test))\n",
    "    sk_vals.append(score_preds(p2, y_test))\n",
    "\n",
    "print(\"Compare our MLP with SKL MLP\")\n",
    "\n",
    "for index in  range(len(test_ratios)):\n",
    "    print(\"Test Ratio: \", [.3, .27, .35, .1, .20][index])\n",
    "    print(\"Accuracy, Precision, Recall, F1_score\")\n",
    "    print(\"MLP: \", mlp_vals[index])\n",
    "    print(\"SKL: \", sk_vals[index])\n",
    "print() \n",
    "print('Average Scores MLP',  np.average(np.array(mlp_vals), axis=0))\n",
    "print('Average Scores skl',  np.average(np.array(sk_vals), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c69e896b73d80df03b10aefe902562c227bdab9e6e1527e46fe261fc763f811"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
